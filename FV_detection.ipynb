{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYk8l+UhIV4dI8EMN3GjJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ohohtani/fake_voice_detection/blob/main/FV_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F_sKla-sZiBS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch                                        # 파이토치의 기본 모듈\n",
        "import torch.nn as nn                               # 신경망\n",
        "import torch.nn.functional as F                     # 신경망에서 자주 사용하는 함수들 ex) 활성화 함수\n",
        "import torch.optim as optim                         # 최적화 알고리즘\n",
        "from torch.utils.data import DataLoader, Dataset    # 데이터 로딩 및 처리\n",
        "from torchvision import transforms                  # 데이터 변환을 위한"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 클래스 정의\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, dataset_path, labels, sample_rate, duration, n_mfcc, max_time_steps, transform=None):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.labels = labels\n",
        "        self.sample_rate = sample_rate\n",
        "        self.duration = duration\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.max_time_steps = max_time_steps\n",
        "        self.transform = transform\n",
        "        self.file_list = list(labels.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.file_list[idx]\n",
        "        file_path = os.path.join(self.dataset_path, file_name + \".flac\")\n",
        "        label = self.labels[file_name]\n",
        "\n",
        "        # 오디오 파일을 로드하고 전처리\n",
        "        audio, _ = librosa.load(file_path, sr=self.sample_rate, duration=self.duration)\n",
        "        mfcc = librosa.feature.mfcc(y=audio, sr=self.sample_rate, n_mfcc=self.n_mfcc)\n",
        "\n",
        "        # 모든 MFCC가 동일한 길이(타임스텝)를 가지도록 설정\n",
        "        if mfcc.shape[1] < self.max_time_steps:\n",
        "            mfcc = np.pad(mfcc, ((0, 0), (0, self.max_time_steps - mfcc.shape[1])), mode='constant')\n",
        "        else:\n",
        "            mfcc = mfcc[:, :self.max_time_steps]\n",
        "\n",
        "        # 변환이 정의되어 있으면 변환 적용\n",
        "        if self.transform:\n",
        "            mfcc = self.transform(mfcc)\n",
        "\n",
        "        # 채널 추가 (1, N_MFCC, 타임스텝)\n",
        "        mfcc = np.expand_dims(mfcc, axis=0)\n",
        "\n",
        "        return torch.tensor(mfcc, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "4Atm3EQgveXR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 경로와 파라미터 정의\n",
        "DATASET_PATH = \"경로 지정\"                          # 데이터셋이 저장된 디렉토리 경로\n",
        "LABEL_FILE_PATH = \"라벨 파일 경로\"                  # 아래 클래스를 나타낼 파일임 (txt)\n",
        "NUM_CLASSES = 2                                     # 클래스 수 (찐이랑 짭)\n",
        "SAMPLE_RATE = 16000                                 # 오디오 파일의 샘플링 레이트 , 16000이 좀 기본적으로 많이 사용하는 느낌 , 이 값을 높임으로써 더 나은 음질을 얻을 수 있으나 계산이 복잡해짐.\n",
        "DURATION = 5                                        # 오디오 클립의 길이 (초 단위) , 5초면 적절함. 다른 분들거 찾아봐도 길이 길어야 6초였음. (3~8초 정도로 바꿔가면서 테스트)\n",
        "N_MFCC = 13                                         # MFCC 계수의 수 (일반적으로 13을 사용)\n",
        "max_time_steps = 156                                # 파동 분석을 위해 신호를 작은 시간 단위로 나눔\n",
        "                                                    # 프레임의 수 = (80000 - 2048) / 512 + 1 ≈ 155.56  (80000은 16000샘플링 * 5초, 2048과 512는 프레임 크기와 홉 기본 값)"
      ],
      "metadata": {
        "id": "lN5DBfAWZogn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = {}                                         # 파일 이름과 라벨을 저장할 딕셔너리\n",
        "\n",
        "with open(LABEL_FILE_PATH, 'r') as label_file:      # 라벨 파일을 열고 각 라인의 내용을 읽음\n",
        "    lines = label_file.readlines()\n",
        "\n",
        "for line in lines:                                  # 각 라인을 처리하여 파일 이름과 라벨을 추출\n",
        "    parts = line.strip().split()                    # 라인을 공백으로 나눔\n",
        "    file_name = parts[1]                            # 파일 이름을 parts[1]에 저장함 parts[0]에는 다른 중요한 정보를 저장하거나 가독성을 위함, 여기서는 Speaker ID로 사용\n",
        "    label = 1 if parts[-1] == \"real\" else 0         # 라벨이 찐이면 1, 아니면 0 (real or fake의 정보는 parts의 마지막 요소에 저장)\n",
        "    labels[file_name] = label                       # 파일 이름과 라벨을 딕셔너리에 저장"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "collapsed": true,
        "id": "0UOdcPpeZorP",
        "outputId": "79c7efd4-9aa4-4da3-f086-cc5d507000bb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '라벨 파일 경로'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1cf38de1f26e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m                                         \u001b[0;31m# 파일 이름과 라벨을 저장할 딕셔너리\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABEL_FILE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlabel_file\u001b[0m\u001b[0;34m:\u001b[0m      \u001b[0;31m# 라벨 파일을 열고 각 라인의 내용을 읽음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '라벨 파일 경로'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 생\n",
        "dataset = AudioDataset(DATASET_PATH, labels, SAMPLE_RATE, DURATION, N_MFCC, max_time_steps)\n",
        "\n",
        "# 데이터셋을 학습 및 검증 세트로 분할\n",
        "dataset_size = len(dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = dataset_size - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "collapsed": true,
        "id": "-d-h2578Zotb",
        "outputId": "d026204a-f231-4713-abc8-7a51e11df4e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "num_samples should be a positive integer value, but got num_samples=0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2cb723bae477>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 데이터셋과 데이터로더 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDURATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_MFCC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 이제 데이터 로더를 사용하여 데이터에 접근할 수 있습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# map-style\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoader를 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "va1NmV5VZovy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN 모델 클래스 정의\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_classes=2, n_mfcc=13, max_time_steps=156):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3))\n",
        "        self.pool = nn.MaxPool2d(kernel_size=(2, 2))\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3))\n",
        "        self.fc1 = nn.Linear(64 * ((n_mfcc // 4) * (max_time_steps // 4)), 128)  # Fully connected layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))  # Conv2D + ReLU\n",
        "        x = self.pool(x)  # MaxPooling2D\n",
        "        x = F.relu(self.conv2(x))  # Conv2D + ReLU\n",
        "        x = self.pool(x)  # MaxPooling2D\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))  # Dense + ReLU\n",
        "        x = self.dropout(x)  # Dropout\n",
        "        x = self.fc2(x)  # Output layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "D0KnWe9JZoyD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초기화\n",
        "model = CNNModel(num_classes=NUM_CLASSES, n_mfcc=N_MFCC, max_time_steps=max_time_steps)\n",
        "\n",
        "# 옵티마이저와 손실 함수 정의\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "6A1_ULimZoz5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 함수 정의\n",
        "def train(model, train_loader, criterion, optimizer, num_epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()  # 기울기 초기화\n",
        "            outputs = model(inputs)  # 모델 출력\n",
        "            loss = criterion(outputs, labels)  # 손실 계산\n",
        "            loss.backward()  # 역전파\n",
        "            optimizer.step()  # 가중치 업데이트\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "# 모델 검증 함수 정의\n",
        "def validate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += torch.sum(preds == labels.data)\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    val_accuracy = correct.double() / len(val_loader.dataset)\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "zwCuPdL8Zo13"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습 및 검증\n",
        "train(model, train_loader, criterion, optimizer, num_epochs=10)\n",
        "validate(model, val_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "DtMHmLAXZo3x",
        "outputId": "fbac800a-72a0-4181-ef87-84c8df679847"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-00ea6862d5d0>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 모델 학습 및 검증\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 저장 함수 정의\n",
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")"
      ],
      "metadata": {
        "id": "rYHzcxiT1njt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(model, \"audio_classifier.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaMQT-GW1nly",
        "outputId": "207118e8-09d9-4817-e7db-8c139d778655"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to audio_classifier.pth\n"
          ]
        }
      ]
    }
  ]
}